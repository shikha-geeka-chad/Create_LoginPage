{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xChkiqlLet"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/NEU-Silicon-Valley/CS7150-CA2/blob/main/CA2.3_CIFAR_classifier.ipynb)\n",
        "\n",
        "# CIFAR-10 Classification (Fully-Connected vs. Convolutional)\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Download **CIFAR-10** (a dataset of 32Ã—32 color images in 10 classes).\n",
        "2. Demonstrate a working classifier using **fully-connected (FC) layers** (a simple MLP).\n",
        "3. **Exercise**: Students will create a **convolutional** version for better efficiency.\n",
        "4. Compare **parameter counts** and performance.\n",
        "\n",
        "<img src=\"https://github.com/NEU-Silicon-Valley/CS7150-CA2/blob/main/cifar10.jpg?raw=1\" width=\"700\"/>\n",
        "\n",
        "This exercise is just an opportunity to understand the power of weight-sharing and play with a standard classification setting that for decades was a focus of machine learning researchers.\n",
        "\n",
        "Try to improve the test performance of the network without making it more expensive to train.  You will just be graded in your experiment findings at the end.\n",
        "\n",
        "**Key Points**:\n",
        "- CIFAR-10 has 60,000 images (50k train, 10k test).\n",
        "- Each image is 3Ã—32Ã—32 (3 color channels).\n",
        "- Weâ€™ll flatten those 3Ã—32Ã—32 = 3072 pixels as input to a fully-connected MLP.\n",
        "- Then weâ€™ll invite you to use convolutional layers, which drastically reduce parameters by sharing weights.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RGQU08SWIyM"
      },
      "source": [
        "## 0. DataLoader Fundamentals: Handling Large-Scale Data Efficiently\n",
        "\n",
        "So far, we've worked with small datasets that can be loaded directly into a single tensor (like in our optimization exercises). However, most real-world datasets, especially in image processing, are far too large to fit into your computer's RAM all at once. Imagine trying to load all 14 million images of the ImageNet datasetâ€”it's impossible for most machines!\n",
        "\n",
        "**The Problem**:\n",
        "- CIFAR-10 has 60,000 images (50k train + 10k test)\n",
        "- Each image is 32Ã—32Ã—3 = 3,072 values\n",
        "- Loading all at once: 50,000 Ã— 3,072 Ã— 4 bytes â‰ˆ 600MB (manageable)\n",
        "- But ImageNet? 14M Ã— 224Ã—224Ã—3 Ã— 4 bytes â‰ˆ 8.4TB! ðŸ¤¯\n",
        "\n",
        "**The Solution**: PyTorch's **DataLoader** - a powerful abstraction that:\n",
        "1. Loads data in small **batches** (e.g., 64 images at a time)\n",
        "2. **Shuffles** data each epoch for better training\n",
        "3. Performs **on-the-fly transformations** (augmentation, normalization)\n",
        "4. Loads data in **parallel** using multiple CPU workers\n",
        "5. Automatically handles the **iteration** over your dataset\n",
        "\n",
        "Let's build intuition step by step:\n",
        "\n",
        "### 0.1 The Dataset Class - Your Data Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mKyzWUDcWIyM",
        "outputId": "248435a2-bfea-4a0c-9131-162382aa6a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 1000 samples\n",
            "Sample 0 shape: image=torch.Size([3, 32, 32]), label=5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Let's create a toy dataset to understand the mechanics\n",
        "class ToyDataset(Dataset):\n",
        "    \"\"\"A simple dataset of random 'images' and labels\"\"\"\n",
        "    def __init__(self, num_samples=1000, image_size=(3, 32, 32)):\n",
        "        # In reality, you'd load file paths or data here\n",
        "        self.num_samples = num_samples\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Tell PyTorch how many samples we have\"\"\"\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return one sample at index idx\n",
        "        This is called by the DataLoader to fetch data\"\"\"\n",
        "        # Simulate loading an image (in practice, you'd read from disk)\n",
        "        fake_image = torch.randn(self.image_size)  # Random \"image\"\n",
        "        fake_label = torch.randint(0, 10, (1,)).item()  # Random label (0-9)\n",
        "        return fake_image, fake_label\n",
        "\n",
        "# Create our toy dataset\n",
        "toy_dataset = ToyDataset(num_samples=1000)\n",
        "print(f\"Dataset has {len(toy_dataset)} samples\")\n",
        "print(f\"Sample 0 shape: image={toy_dataset[0][0].shape}, label={toy_dataset[0][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CAsWcpwWIyN"
      },
      "source": [
        "### 0.2 The DataLoader - Your Batch Manager\n",
        "\n",
        "Now that we have a Dataset, let's see how DataLoader transforms it into efficient batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eHkwsNiXWIyN",
        "outputId": "baa7fcac-b676-4837-baf4-718fb74d6a00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: images shape=torch.Size([32, 3, 32, 32]), labels shape=torch.Size([32])\n",
            "Batch 1: images shape=torch.Size([32, 3, 32, 32]), labels shape=torch.Size([32])\n",
            "Batch 2: images shape=torch.Size([32, 3, 32, 32]), labels shape=torch.Size([32])\n",
            "\n",
            "Total batches: 32 = 1000 samples Ã· 32 batch_size\n"
          ]
        }
      ],
      "source": [
        "# Create a DataLoader with different settings\n",
        "toy_loader = DataLoader(\n",
        "    toy_dataset,\n",
        "    batch_size=32,      # Process 32 samples at once\n",
        "    shuffle=True,       # Randomize order each epoch\n",
        "    num_workers=0       # Use main process (set >0 for parallel loading)\n",
        ")\n",
        "\n",
        "# Let's examine what the DataLoader gives us\n",
        "for batch_idx, (images, labels) in enumerate(toy_loader):\n",
        "    print(f\"Batch {batch_idx}: images shape={images.shape}, labels shape={labels.shape}\")\n",
        "    if batch_idx == 2:  # Just show first 3 batches\n",
        "        break\n",
        "\n",
        "print(f\"\\nTotal batches: {len(toy_loader)} = {len(toy_dataset)} samples Ã· {32} batch_size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HQkQkcWIyN"
      },
      "source": [
        "### 0.3 Key Takeaways\n",
        "\n",
        "âœ… **Dataset** defines how to access your data (one sample at a time) </br>\n",
        "âœ… **DataLoader** = Batches multiple samples together efficiently </br>\n",
        "âœ… **Batch size** = How many samples to process together (larger = more stable gradients) </br>\n",
        "âœ… **Shuffle** = Randomizes order each epoch; prevents the model from memorizing data order (crucial for good training!) </br>\n",
        "âœ… **num_workers** = Parallel CPU threads for loading (*0* = main thread, *>0* = faster loading, but more RAM) </br>\n",
        "\n",
        "**Memory Rule of Thumb**:\n",
        "- Batch size Ã— Sample size Ã— 4 bytes (float32) Ã— ~3 (for gradients) = GPU memory needed\n",
        "- Example: 64 images Ã— 3Ã—224Ã—224 Ã— 4 bytes Ã— 3 â‰ˆ 115MB per batch\n",
        "\n",
        "**Why this matters:**\n",
        "- **Memory efficient**: Only loads what fits in GPU memory\n",
        "- **Better gradients**: Averaging over batches reduces noise\n",
        "- **Faster training**: GPU parallelism works best with batches\n",
        "- **Shuffling prevents overfitting**: Model can't memorize data order\n",
        "\n",
        "In our main assignment code, `torchvision.datasets.CIFAR10(...)` is simply a more complex, pre-built `Dataset` provided by PyTorch that knows how to load CIFAR-10 images from disk one by one. The `DataLoader` then wraps it to give us the shuffled mini-batches we need for training.\n",
        "\n",
        "Now that we understand how the data is loaded, let's set up the pipeline for our CIFAR-10 experiment.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_w9x5kRlLeu"
      },
      "source": [
        "## 1. Setup\n",
        "We'll import **PyTorch**, **torchvision**, then load CIFAR-10. Weâ€™ll make small transformations (convert to tensors, normalize if desired)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UKdeg3nZlLev",
        "outputId": "a40bc1e7-f0e1-45d4-fa95-1587bf78f9c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:03<00:00, 46.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Basic transforms: ToTensor (range [0,1]), optional normalization.\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Optionally normalize: T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Download and create datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhVBc1grlLev"
      },
      "source": [
        "## 2. A Simple Fully-Connected (MLP) Classifier\n",
        "Weâ€™ll define a basic MLP:\n",
        "1. Flatten the 3Ã—32Ã—32 image (3072 dims).\n",
        "2. Several **fully connected layers**, then 10 outputs (one per CIFAR-10 class).\n",
        "\n",
        "We can train it for a few epochsâ€”**this won't achieve high accuracy** (CNNs do much better), but it demonstrates the approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IrzF_nJXlLev",
        "outputId": "9d857713-233e-43a0-bd55-82ee15d04de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP parameter count: 308310\n"
          ]
        }
      ],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10):\n",
        "        super().__init__()\n",
        "        # A small 2-layer MLP:\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        # x: shape (batch, 3, 32, 32)\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "mlp = SimpleMLP().to(device)\n",
        "print(\"MLP parameter count:\", sum(p.numel() for p in mlp.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcCfMLLlLev"
      },
      "source": [
        "### 2.1 Training Loop\n",
        "We define a simple function `train_epoch` and `test_accuracy` to measure performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uoexYXvIlLev"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def test_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "            predicted = preds.argmax(dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100.0 * correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5vL1-elLev"
      },
      "source": [
        "Now let's do a short training run on the MLPâ€”**note** that this won't get anywhere close to SOTA accuracy on CIFAR-10, but it demonstrates the pipeline. We'll do maybe **2** or **3** epochs just to see it learns something."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xonWX3m5lLev",
        "outputId": "d0f87120-e999-46cb-b8e5-c5b0d8ca8888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, train loss=1.8821, test acc=34.21%\n",
            "Epoch 2/3, train loss=1.7298, test acc=41.10%\n",
            "Epoch 3/3, train loss=1.6628, test acc=41.79%\n"
          ]
        }
      ],
      "source": [
        "mlp = SimpleMLP().to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 3  # can increase if you want\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = train_epoch(mlp, train_loader, optimizer)\n",
        "    test_acc = test_accuracy(mlp, test_loader)\n",
        "    print(f\"Epoch {epoch}/{epochs}, train loss={train_loss:.4f}, test acc={test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU6VLKrIlLew"
      },
      "source": [
        "## 3. Exercise: Use a Stack of Convolutions\n",
        "\n",
        "CIFAR-10 was **designed** with 2D images in mind, so we can do **far better** with **convolutional** layers that share weights locally.\n",
        "\n",
        "### Your Tasks\n",
        "1. **Construct** a new network (say `ConvNet`) with multiple convolutional layers, optional pooling, etc.\n",
        "2. **Count** the number of parameters. *(Hint: `sum(p.numel() for p in model.parameters() if p.requires_grad)`.)*\n",
        "3. **Train** this model on CIFAR-10. Try to achieve comparable or better accuracy than the MLP **with fewer parameters**.\n",
        "\n",
        "### Suggested Skeleton Code\n",
        "Below is a minimal skeleton. Feel free to modify layer dimensions, add pooling, or add more conv layers. We provide the class structure for you to fill in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fS5CZ28XlLew"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Starting simple but effective\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)    # (3,32,32) -> (16,32,32)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)   # (16,32,32) -> (32,32,32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)                              # Reduces size by half\n",
        "\n",
        "        # After conv1 -> conv2 -> pool: (32,16,16)\n",
        "        # After another conv and pool: (32,8,8)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1)    # (32,16,16) -> (32,16,16)\n",
        "\n",
        "        # Final classifier - calculate the flattened size correctly\n",
        "        # After 2 pooling operations: 32x32 -> 16x16 -> 8x8\n",
        "        # So we have 32 channels Ã— 8 Ã— 8 = 2048 features\n",
        "        self.fc = nn.Linear(32 * 8 * 8, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 3, 32, 32)\n",
        "        x = F.relu(self.conv1(x))           # (batch, 16, 32, 32)\n",
        "        x = F.relu(self.conv2(x))           # (batch, 32, 32, 32)\n",
        "        x = self.pool(x)                    # (batch, 32, 16, 16)\n",
        "        x = F.relu(self.conv3(x))           # (batch, 32, 16, 16)\n",
        "        x = self.pool(x)                    # (batch, 32, 8, 8)\n",
        "\n",
        "        # Flatten for the fully connected layer\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)          # (batch, 2048)\n",
        "        x = self.fc(x)                      # (batch, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLbtVVyNlLew"
      },
      "source": [
        "### 3.1 Code: Train Your ConvNet\n",
        "**Exercise**: Implement the training loop (similar to the MLP), measure test accuracy, and see how you can reduce or increase parameters to trade off accuracy vs. model size.\n",
        "\n",
        "Examples:\n",
        "- Add more conv layers or channels.\n",
        "- Add more or fewer pooling layers.\n",
        "- Print out the param count.\n",
        "- Play with other architectural tricks such as residual connections.\n",
        "- Tweak the learning rate or optimizer.\n",
        "\n",
        "Try to see how low you can go in param count while maintaining a decent accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9D9VcwBlLew",
        "outputId": "2021dce7-7f56-4cfc-ad66-2887b2bd5dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet param count: 34,826\n"
          ]
        }
      ],
      "source": [
        "# STUDENT EXERCISE - Architecture 1:\n",
        "convnet = ConvNet().to(device)\n",
        "conv_params = sum(p.numel() for p in convnet.parameters() if p.requires_grad)\n",
        "print(f\"ConvNet param count: {conv_params:,}\")\n",
        "\n",
        "optimizer_conv = optim.Adam(convnet.parameters(), lr=1e-3)\n",
        "epochs_conv = 3\n",
        "for epoch in range(1, epochs_conv+1):\n",
        "    train_loss = train_epoch(convnet, train_loader, optimizer_conv)\n",
        "    test_acc = test_accuracy(convnet, test_loader)\n",
        "    print(f\"[ConvNet] Epoch {epoch}/{epochs_conv}, train loss={train_loss:.4f}, test acc={test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_AGuN8_lLew"
      },
      "source": [
        "## 4. Report Your Findings\n",
        "\n",
        "Points to understand:\n",
        "\n",
        "1. A **fully-connected** approach to image classification (such as CIFAR-10) can work but tends to have **many** parameters (e.g., 3,072Ã—100 just in one layer on tiny images) and typically yields lower accuracy compared to modern **Convolutional** architectures.\n",
        "2. **Convolution** drastically reduces parameter counts via **weight sharing**, can often achieve much higher accuracy on image tasks, and is typically *translation-equivariant*.\n",
        "3. Your goal is to **experiment** with different conv net designs to minimize param count while maximizing accuracy.\n",
        "\n",
        "Report here at least two iterations of your architectural experiments:\n",
        "\n",
        "1. Using an architecture consisting of $\\fbox{your answer}$, I was able to reduce the parameterization to $\\fbox{your answer}$ parameters and achieve test accuracy of $\\fbox{your answer}$ after three epochs of training.\n",
        "\n",
        "2. In a second test, I tried an architecture consisting of $\\fbox{your answer}$.  That used an even smaller parameterization, with only $\\fbox{your answer}$ parameters, and it achieved test accuracy of $\\fbox{your answer}$ after three epochs of training.\n",
        "\n",
        "Good luck!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python"
    },
    "name": "CIFAR10_Classifier_FC_vs_Conv"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}